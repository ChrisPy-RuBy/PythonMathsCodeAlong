{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Numerical Derivatives - Finite Difference Approximations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of ways to calculate numerical derivatives. The most commonly used methods belong to a group called the *finite difference approximations*. As the name suggests, these methods approximate the gradient of a function at a point x0, f'(x0), using the values of the function at that point f(x0), and at nearby points f(x0 +/- delta), f(x0 +/- 2\\*delta) etc... Here, for accurate results, it is essential that delta is *small*.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's investigate the situation where we have some trial function that we want to differentiate numerically, and explore some finite difference approximations for the derivative. We'll choose a trial function that we can differentiate analytically, so we have exact derivatives against which to compare the results of the numerical methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our trial function and analytic derivatives**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.190199"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we'll use a high order polynomial\n",
    "f = lambda x: 0.1*x**5 - 0.2*x**3 + 0.1*x - 0.2\n",
    "# We're interested in the value at x = 0.1\n",
    "f(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09405000000000001"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For comparison below, here is the analytic first derivative\n",
    "df_dx = lambda x: 0.5*x**4 - 0.6*x**2 + 0.1\n",
    "df_dx(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.118"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And here is the analytic second derivative\n",
    "d2f_dx2 = lambda x: 2.0*x**3 - 1.2*x\n",
    "d2f_dx2(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forward difference approximation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the name suggests, this method calculates the derivative at x using the function at x,\n",
    "# and the function at x+delta, i.e. a 'forward' looking difference\n",
    "def forward_first(f, x, delta=0.001):\n",
    "    return (f(x+delta) - f(x)) / delta\n",
    "\n",
    "# For the second derivative, we're still looking forward, but now we need the next two points\n",
    "def forward_second(f, x, delta=0.001):\n",
    "    return (f(x+2*delta) - 2*f(x+delta) + f(x)) / delta**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Central difference approximation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The central first derivative uses a point 'behind' and a point 'ahead', so it's 'centred' on the point x of interest \n",
    "def central_first(f, x, delta=0.001):\n",
    "    return (f(x+delta) - f(x-delta)) / (2*delta)\n",
    "\n",
    "# Ditto for the central second derivative: a point behind, the central point, and a point ahead. \n",
    "def central_second(f, x, delta=0.001):\n",
    "    return (f(x+delta) - 2*f(x) + f(x-delta)) / delta**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backward difference approximation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To all intents and purposes, the backward difference is just the forward difference in reverse.\n",
    "def backward_first(f, x, delta=0.001):\n",
    "    return (f(x) - f(x-delta)) / delta\n",
    "\n",
    "def backward_second(f, x, delta=0.001):\n",
    "    return (f(x) - 2*f(x-delta) + f(x-2*delta)) / delta**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward first 0.09063187500000014\n",
      "Forward second -0.17287500000000564\n",
      "Central first 0.093575625\n",
      "Central second -0.11774999999999422\n",
      "Backward first 0.09651937499999985\n",
      "Backward second -0.059625000000007575\n"
     ]
    }
   ],
   "source": [
    "delta = 0.05\n",
    "x = 0.1\n",
    "print(\"Forward first\", forward_first(f, x, delta))\n",
    "print(\"Forward second\", forward_second(f, x, delta))\n",
    "print(\"Central first\", central_first(f, x, delta))\n",
    "print(\"Central second\", central_second(f, x, delta))\n",
    "print(\"Backward first\", backward_first(f, x, delta))\n",
    "print(\"Backward second\", backward_second(f, x, delta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Play around with this for a while.** Which of the three methods yields a better approximation to the analytic derivatives? How does the accuracy vary with delta?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Size of Errors**<br>\n",
    "Theory suggests that the backward and forward first derivatives should have errors of 'order delta', also written as O(delta), while the central first derivative has an error O(delta^2). So, if delta is *small* (i.e. less than 1), it is clear that delta^2 is *smaller* than delta, and we expect the central difference approximation to be more accurate. Do your results bear this out? A similar analysis suggests that the central second derivative should be more accurate than the forward second and backward second derivatives. Does this seem to be true?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
